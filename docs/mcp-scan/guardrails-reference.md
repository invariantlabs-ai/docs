---
title: Advanced Guardrails
description: Complete reference for writing guardrailing rules with Invariant.
icon: bootstrap/book
---

# Advanced Guardrails

This document contains the complete Invariant Guardrails documentation for writing custom guardrailing rules.

---

## Table of Contents

1. [Introduction: Securing Agents with Rules](#introduction-securing-agents-with-rules)
2. [Agents and Traces](#agents-and-traces)
3. [Rule Writing Reference](#rule-writing-reference)
4. [Agent Guardrails](#agent-guardrails)
   - [Tool Calls](#tool-calls)
   - [Loop Detection](#loop-detection)
   - [Dataflow Rules](#dataflow-rules)
   - [Code Validation](#code-validation)
5. [Content Guardrails](#content-guardrails)
   - [PII Detection](#pii-detection)
   - [Jailbreaks and Prompt Injections](#jailbreaks-and-prompt-injections)
   - [Images](#images)
   - [Moderated and Toxic Content](#moderated-and-toxic-content)
   - [Regex Filters](#regex-filters)
   - [Copyrighted Content](#copyrighted-content)
   - [Secret Tokens and Credentials](#secret-tokens-and-credentials)
   - [Sentence Similarity](#sentence-similarity)
   - [LLM-as-Guardrail](#llm-as-guardrail)

---

# Introduction: Securing Agents with Rules

Learn the fundamentals about guardrailing with Invariant.

Guardrailing agents can be a complex undertaking, as it involves understanding the entirety of your agent's potential behaviors and misbehaviors.

This chapter covers the fundamentals of guardrailing with Invariant, with a primary focus on how Invariant allows you to write both strict and fuzzy rules that precisely constrain your agent's behavior.

## Understanding Your Agent's Capabilities

Before securing an agent, it is important to understand its capabilities. This includes understanding the tools and functions available to the agent, along with the parameters it can accept. For instance, you may want to consider whether it has access to private or sensitive data, the ability to send emails, or the authority to perform destructive actions such as deleting files or initiating payments.

This is important to understand, as it forms the basis for threat modeling and risk assessment. In contrast to traditional software, agentic systems are highly dynamic, meaning tools and APIs can be called in arbitrary ways, and the agent's behavior can change based on the context and the task at hand.

## Constraining Your Agent's Capability Space with Rules

Once you have a good understanding of your agent's capabilities, you can start writing rules to constrain its behavior. By defining guardrails, you limit the agent's behavior to a safe and intended subset of its full capabilities. These rules can specify allowed tool calls, restrict parameter values, enforce order of operations, and prevent destructive looping behaviors.

Invariant's guardrailing runtime allows you to express these constraints declaratively, ensuring the agent only operates within predefined security boundariesâ€”even in dynamic and open-ended environments. This makes it easier to detect policy violations, reduce risk exposure, and maintain trust in agentic systems.

## Writing Your First Rule

Let's assume a simple example agent capable of managing a user's email inbox. Such an agent may be configured with two tools:

- `get_inbox()` to check a user's inbox and read the emails
- `send_email(recipient: str, subject: str, body: str)` to send an email to a user.

Unconstrained, this agent can easily fail, allowing a bad actor or sheer malfunction to induce failure states such as data leaks, spamming, or even phishing attacks.

To prevent this, we can write a set of simple guardrailing rules, to harden our agent's security posture and limit its capabilities.

### Example 1: Constraining an email agent with guardrails

Let's begin by writing a simple rule that prevents the agent from sending emails to untrusted recipients.

```python
# ensure we know all recipients
raise "Untrusted email recipient" if:
    (call: ToolCall)
    call is tool:send_email
    not match(".*@company.com", call.function.arguments.recipient)
```

This simple rule demonstrates Invariant's guardrailing rules: To prevent certain agent behavior, we write detection rules that match instances of undesired behavior.

In this case, we want to prevent the agent from sending emails to untrusted recipients. We do so by describing a tool call that would violate our policy, and then raising an error if such a call is detected.

This way of writing guardrails _decouples guardrailing and security rules from core agent logic_. This is a key concept with Invariant and it allows you to write and maintain them independently. It also means security and agent logic can be maintained by different teams, and that security rules can be deployed and updated independently of the agent system.

### Example 2: Constraining agent flow

Next, let's also consider different workflows that our agent may carry out. For example, our agent may first check the user's inbox and then decide to send an email.

This behavior has the risk that the agent may be prompt injected by an untrusted email, leading to malicious behavior.

To prevent this, we can write a simple flow rule, that not only checks specific tool calls, but also considers the data flow of the agent, i.e. what the agent has previously done and ingested before it decided to take action:

```python
from invariant.detectors import prompt_injection, moderated

raise "Must not send an email when agent has looked at suspicious email" if:
    (inbox: ToolOutput) -> (call: ToolCall)
    inbox is tool:get_inbox
    call is tool:send_email
    prompt_injection(inbox.content)
```

This rule checks if the agent has looked at a suspicious email, and if so, it raises an error when the agent tries to send an email. It does so by defining a two-part pattern, consisting of a tool call and a tool output.

Our rule triggers when, first, we ingest the output of the `get_inbox` tool, and then we call the `send_email` tool. This is expressed by the `(inbox: ToolOutput) -> (call: ToolCall)` pattern, which matches the data flow of the agent.

---

# Agents and Traces

Learn about Guardrails primitives to model agent behavior for guardrailing.

Invariant uses a simple yet powerful event-based trace model of agentic interactions, derived from the OpenAI chat data structure.

## Agent Trace

An agent trace is a sequence of events generated by an agent during a multi-turn interaction or reasoning process. It consists of a sequence of `Event` objects, each being concretized as one of the classes defined below (`Message`, `ToolCall`, `ToolOutput`, etc.).

In a guardrailing rule, you can then use these types, to quantify and check your agentic traces for behaviors:

```python
raise "Found pattern" if:
    (msg: Message) # <- checks every agent message (user, system, assistant)

    (call: ToolCall) # <- checks every tool call

    (output: ToolOutput) # <- checks every tool output

    # actual rule logic
```

## Data Model

### Message

```python
class Message(Event):
    role: str
    content: Optional[str] | list[Content]
    tool_calls: Optional[list[ToolCall]]

class Content:
    type: str

class TextContent(Content):
    type: str = "text"
    text: str

class ImageContent(Content):
    type: str = "image"
    image_url: str
```

**Fields:**

- `role` (string, required): The role of the event, e.g., `user`, `assistant`, `system`
- `content` (string | list[Content], optional): The content of the event
- `tool_calls` (list[ToolCall], optional): A list of tool calls made by the agent

**Example - Simple message:**

```json
{ "role": "user", "content": "Hello, how are you?" }
```

**Example - Message with tool call:**

```json
{
  "role": "assistant",
  "content": "Checking your inbox...",
  "tool_calls": [
    {
      "id": "1",
      "type": "function",
      "function": {
        "name": "get_inbox",
        "arguments": { "n": 10 }
      }
    }
  ]
}
```

### ToolCall

```python
class ToolCall:
    id: str
    type: str
    function: Function

class Function:
    name: str
    arguments: dict
```

**Fields:**

- `id` (string, required): A unique identifier for the tool call
- `type` (string, required): The type of the tool call, e.g., `function`
- `function` (Function, required): The function call made by the agent
  - `name` (string, required): The name of the function called
  - `arguments` (dict, required): The arguments passed to the function

### ToolOutput

```python
class ToolOutput(Message):
    role: str
    content: str | list[Content]
    tool_call_id: Optional[str]
```

**Fields:**

- `role` (string, required): The role of the event, e.g., `tool`
- `content` (string, required): The content of the tool output
- `tool_call_id` (string, optional): The identifier of a previous ToolCall

### Full Trace Example

```json
[
  { "role": "user", "content": "What's in my inbox?" },
  {
    "role": "assistant",
    "content": "Here are the latest emails.",
    "tool_calls": [
      {
        "id": "1",
        "type": "function",
        "function": { "name": "get_inbox", "arguments": {} }
      }
    ]
  },
  {
    "role": "tool",
    "tool_call_id": "1",
    "content": "1. Subject: Hello, From: Alice, Date: 2024-01-01"
  },
  { "role": "assistant", "content": "You have 1 new email." }
]
```

---

# Rule Writing Reference

A concise reference for writing guardrailing rules with Invariant.

## Message-Level Guardrails

**Example: Checking for specific keywords**

```python
raise "The one who must not be named" if:
    (msg: Message)
    "voldemort" in msg.content.lower() or "tom riddle" in msg.content.lower()
```

**Example: Checking for prompt injections**

```python
from invariant.detectors import prompt_injection

raise "Prompt injection detected" if:
    (msg: Message)
    prompt_injection(msg.content)
```

## Tool Call Guardrails

**Example: Matching a specific tool call**

```python
raise "Must not send any emails to Alice" if:
    (call: ToolCall)
    call is tool:send_email({ to: "alice@mail.com" })
```

**Example: Regex matching on tool parameters**

```python
raise "Must not send any emails to <anyone>@disallowed.com" if:
    (call: ToolCall)
    call is tool:send_email({ to: r".*@disallowed.com" })
```

**Example: PII in tool output**

```python
from invariant.detectors import pii

raise "PII in tool output" if:
    (out: ToolOutput)
    len(pii(out.content)) > 0
```

## Code Guardrails

**Example: Validating function calls in code**

```python
from invariant.detectors.code import python_code

raise "'eval' function must not be used in generated code" if:
    (msg: Message)
    program := python_code(msg.content)
    "eval" in program.function_calls
```

**Example: Preventing unsafe bash commands**

```python
from invariant.detectors import semgrep

raise "Dangerous pattern detected in bash command" if:
    (call: ToolCall)
    call is tool:cmd_run
    semgrep_res := semgrep(call.function.arguments.command, lang="bash")
    any(semgrep_res)
```

## Content Guardrails

**Example: Detecting any PII**

```python
from invariant.detectors import pii

raise "Found PII in message" if:
    (msg: Message)
    any(pii(msg))
```

**Example: Detecting credit card numbers**

```python
from invariant.detectors import pii

raise "Found credit card information" if:
    (msg: ToolOutput)
    any(pii(msg, ["CREDIT_CARD"]))
```

**Example: Detecting copyrighted content**

```python
from invariant.detectors import copyright

raise "found copyrighted code" if:
    (msg: Message)
    not empty(copyright(msg.content, threshold=0.75))
```

## Data Flow Guardrails

**Example: Preventing a simple flow**

```python
raise "Must not call tool after user uses keyword" if:
    (msg: Message) -> (tool: ToolCall)
    msg.role == "user"
    "send" in msg.content
    tool is tool:send_email
```

**Example: Preventing sensitive data leaks**

```python
from invariant.detectors import secrets

raise "Must not leak secrets externally" if:
    (msg: Message) -> (tool: ToolCall)
    len(secrets(msg.content)) > 0
    tool.function.name in ["create_pr", "add_comment"]
```

## Loop Detection

**Example: Limiting tool call count**

```python
from invariant import count

raise "Allocated too many virtual machines" if:
    count(min=3):
        (call: ToolCall)
        call is tool:allocate_virtual_machine
```

**Example: Detecting retry loops**

```python
from invariant import count

raise "Repetition of length in [2,10]" if:
    (call1: ToolCall)
    call1 is tool:check_status
    count(min=2, max=10):
        call1 -> (other_call: ToolCall)
        other_call is tool:check_status
```

---

# Agent Guardrails

## Tool Calls

Guardrail the function and tool calls of your agentic system.

At the core of any agentic system are function and tool calls, i.e. the ability for the agent to interact with the environment via designated functions and tools.

For security reasons, it is important to ensure that all tool calls an agent executes are validated and well-scoped, to prevent undesired or harmful actions.

### Tool Calling Risks

Since tools are an agent's interface to interact with the world, they can also be used to perform actions that are harmful or undesired. For example, an insecure agent could:

- Leak sensitive information, e.g. via a `send_email` function.
- Delete an important file, via a `delete_file` or a `bash` command.
- Make a payment to an attacker.
- Send a message to a user with sensitive information.

### Preventing Tool Calls

To match a specific tool call in a guardrailing rule, you can use `call is tool:<tool_name>` expressions.

```python
raise "Must not send any emails" if:
    (call: ToolCall)
    call is tool:send_email
```

### Preventing Specific Tool Call Parameterizations

Tool calls can also be matched by their parameters:

```python
raise "Must not send any emails to Alice" if:
    (call: ToolCall)
    call is tool:send_email({ to: "alice@mail.com" })
```

### Regex Matching

```python
raise "Must not send any emails to <anyone>@disallowed.com" if:
    (call: ToolCall)
    call is tool:send_email({ to: r".*@disallowed.com" })
```

### Content Matching

```python
raise "Must not send any emails with locations" if:
    (call: ToolCall)
    call is tool:send_email({ body: <LOCATION> })
```

This type of content matching also works for: `EMAIL_ADDRESS`, `LOCATION`, `PHONE_NUMBER`, `PERSON`, `MODERATED`.

### Checking Tool Outputs

```python
from invariant.detectors import pii

raise "PII in tool output" if:
    (out: ToolOutput)
    len(pii(out.content)) > 0
```

### Checking only certain tool outputs

```python
from invariant.detectors import moderated

raise "Moderated content in tool output" if:
    (out: ToolOutput)
    out is tool:read_website
    moderated(out.content, cat_thresholds={"hate/threatening": 0.1})
```

### Checking classes of tool calls

```python
raise "Banned tool used" if:
    (call: ToolCall)
    call.function.name in ["send_email", "delete_file"]
```

---

## Loop Detection

Detect and prevent infinite loops in your agentic system.

### Looping Risks

Loops are a common source of bugs and errors in agentic systems:

- Get stuck in an infinite loop, consuming resources and causing the system to crash
- Get stuck in a loop that causes irreversible actions (e.g., sending a message many times)
- Get stuck in a loop requiring many expensive LLM calls

### Limiting Number of Uses for Certain Operations

```python
from invariant import count

raise "Allocated too many virtual machines" if:
    count(min=3):
        (call: ToolCall)
        call is tool:allocate_virtual_machine
```

### Retry Loops

```python
raise "3 retries of check_status" if:
    (call1: ToolCall) -> (call2: ToolCall)
    call2 -> (call3: ToolCall)
    call1 is tool:check_status
    call2 is tool:check_status
    call3 is tool:check_status
```

### Detecting Loops with Quantifiers

```python
from invariant import count

raise "Repetition of length in [2,10]" if:
    (call1: ToolCall)
    call1 is tool:check_status
    count(min=2, max=10):
        call1 -> (other_call: ToolCall)
        other_call is tool:check_status
```

---

## Dataflow Rules

Secure the dataflow of your agentic system, to ensure that sensitive data never leaves the system through unintended channels.

### Dataflow Risks

Agentic systems can easily leak sensitive information:

- Leak API keys or passwords to an external service
- Send user data or PII to an external service
- Be prompt-injected by an external service to perform malicious actions

### The Flow Operator `->`

The flow operator enables you to detect flows and ordering of operations:

```python
raise "Must not call tool after user uses keyword" if:
    (msg: Message) -> (tool: ToolCall)
    msg.role == "user"
    "send" in msg.content
    tool is tool:send_email
```

### Multi-Turn Flows

```python
raise "Must not call tool after user uses keyword" if:
    (msg: Message) -> (tool: ToolCall)
    tool -> (output: ToolOutput)
    msg.role == "user"
    "send" in msg.content
    tool is tool:send_email
    "Peter" in output.content
```

### Direct Succession Flows `~>`

The `~>` operator specifies direct succession flows (flows of length 1):

```python
raise "Must not call tool after user uses keyword" if:
    (call: ToolCall) ~> (output: ToolOutput)
    call is tool:send_email
    "Peter" in output.content
```

### Combining Content Guardrails with Dataflow Rules

```python
from invariant.detectors import secrets

raise "Must not leak secrets externally" if:
    (msg: Message) -> (tool: ToolCall)
    len(secrets(msg.content)) > 0
    tool.function.name in ["create_pr", "add_comment"]
```

---

## Code Validation

Secure the code that your agent generates and executes.

### Code Validation Risks

- Generate code that contains security vulnerabilities (SQL injection, XSS)
- Generate code that contains bugs causing crashes
- Produce code that escapes a sandboxed execution environment
- Generate code that doesn't follow best practices

### python_code

```python
def python_code(
    data: Union[str, List[str]],
    ipython_mode: bool = False
) -> PythonDetectorResult
```

**PythonDetectorResult fields:**

- `.imports` - list of imported modules
- `.builtins` - list of built-in functions used
- `.syntax_error` - boolean indicating syntax errors
- `.syntax_error_exception` - exception message if syntax error
- `.function_calls` - set of function call names

**Example:**

```python
from invariant.detectors.code import python_code

raise "'eval' function must not be used" if:
    (msg: Message)
    program := python_code(msg.content)
    "eval" in program.function_calls
```

### semgrep

```python
def semgrep(
    data: str | list | dict,
    lang: str
) -> List[CodeIssue]
```

**Example:**

```python
from invariant.detectors import semgrep

raise "Dangerous pattern detected" if:
    (call: ToolCall)
    call is tool:ipython_run_cell
    semgrep_res := semgrep(call.function.arguments.code, lang="python")
    any(semgrep_res)
```

---

# Content Guardrails

## PII Detection

Detect and manage PII in traces.

### PII Risks

Without PII safeguards an insecure agent may:

- Log PII in traces, leading to compliance violations
- Expose PII unintentionally (e.g., in emails)
- Store PII in unqualified storage systems
- Violate GDPR, CCPA, and other regulations

### pii

```python
def pii(
    data: Union[str, List[str]],
    entities: Optional[List[str]]
) -> List[str]
```

**Parameters:**

- `data` - A single message or list of messages
- `entities` - List of PII entity types to detect (defaults to all)

**Returns:** List of detected PII

**Example - Detecting any PII:**

```python
from invariant.detectors import pii

raise "Found PII in message" if:
    (msg: Message)
    any(pii(msg))
```

**Example - Detecting credit cards:**

```python
from invariant.detectors import pii

raise "Found Credit Card information" if:
    (msg: ToolOutput)
    any(pii(msg, ["CREDIT_CARD"]))
```

**Example - Preventing PII leakage:**

```python
from invariant.detectors import pii

raise "Attempted to send PII in an email" if:
    (out: ToolOutput) -> (call: ToolCall)
    any(pii(out.content))
    call is tool:send_email({ to: "^(?!.*@ourcompany.com$).*$" })
```

---

## Jailbreaks and Prompt Injections

Protect agents from being manipulated through indirect or adversarial instructions.

### Prompt Injection Risks

Without prompt injection defenses, agents may:

- Execute tool calls based on deceptive content from external sources
- Obey malicious user instructions that override safety prompts
- Expose private or sensitive information
- Accept inputs that subvert system roles

### prompt_injection

```python
def prompt_injection(data: str | list[str]) -> bool
```

Returns TRUE if a prompt injection was detected.

> **Important:** Classifier-based detection is only a heuristic. Also apply data flow controls and tool call scoping.

**Example:**

```python
from invariant.detectors import prompt_injection

raise "detected an indirect prompt injection" if:
    (out: ToolOutput) -> (call: ToolCall)
    prompt_injection(out.content)
    call is tool:send_email({ to: "^(?!.*@ourcompany.com$).*$" })
```

### unicode

```python
def unicode(
    data: str | list[str],
    categories: list[str] | None = None
) -> list[str]
```

Detects specific types of Unicode characters (e.g., invisible characters used in attacks).

**Example:**

```python
from invariant.detectors import unicode

raise "Found private use control character" if:
    (msg: ToolOutput)
    any(unicode(msg, ["Co"]))
```

---

## Images

Secure images given to or produced by your agentic system.

### Image Risks

- Capture PII like names or addresses
- View credentials such as passwords or API keys
- Get prompt injected from text in an image
- Generate images with explicit or harmful content

### ocr

```python
def ocr(
    data: str | list[str],
    config: dict | None = None
) -> list[str]
```

Extracts text from images using Tesseract.

**Example:**

```python
from invariant.detectors import prompt_injection
from invariant.parsers import ocr

raise "Found Prompt Injection in Image" if:
    (msg: Image)
    ocr_results := ocr(msg)
    prompt_injection(ocr_results)
```

### image

```python
def image(content: Content | list[Content]) -> list[ImageContent]
```

Extracts all ImageContent from mixed content messages.

---

## Moderated and Toxic Content

Defining and enforcing content moderation in agentic systems.

### Moderation Risks

Without moderation safeguards, agents may:

- Generate or amplify hate speech, harassment, or explicit content
- Act on inappropriate user inputs causing unintended behavior
- Spread misinformation or reinforce harmful stereotypes

### moderated

```python
def moderated(
    data: str | list[str],
    model: str | None = None,
    default_threshold: float | None = 0.5,
    cat_threshold: dict[str, float] | None = None
) -> bool
```

**Parameters:**

- `data` - A single message or list of messages
- `model` - Model to use (`KoalaAI/Text-Moderation` or `openai`)
- `default_threshold` - Score threshold (default 0.5)
- `cat_threshold` - Category-specific thresholds

**Example:**

```python
from invariant.detectors import moderated

raise "Detected a harmful message" if:
    (msg: Message)
    moderated(msg.content)
```

**Example with thresholding:**

```python
from invariant.detectors import moderated

raise "Detected a harmful message" if:
    (msg: Message)
    moderated(msg.content, cat_thresholds={"hate/threatening": 0.15})
```

---

## Regex Filters

Use regular expressions to filter messages.

### Plain Text Content Risks

- Generate phishing URLs
- Reference competitors in responses
- Produce content in unsupported output formats
- Use URL smuggling to bypass security measures

### match

```python
def match(pattern: str, content: str) -> bool
```

Wraps `re.match` from Python's standard library.

**Example:**

```python
raise "Must not link to example.com" if:
    (msg: Message)
    match("https?://[^\s]+", msg.content)
```

### find

```python
def find(pattern: str, content: str) -> list[str]
```

Finds all occurrences of a pattern.

**Example:**

```python
raise "must not send emails to anyone but 'Peter'" if:
    (msg: Message)
    (name: str) in find("[A-Z][a-z]*", msg.content)
    name in ["Peter", "Alice", "John"]
```

---

## Copyrighted Content

Copyright compliance in agentic systems.

### Copyright Risks

- Handle, process, and reproduce copyrighted material without permission
- Unknowingly host copyrighted material
- Expose copyrighted material to users

### copyright

```python
def copyright(data: str | list[str]) -> list[str]
```

Returns list of detected copyright types (e.g., `["GNU_AGPL_V3", "MIT_LICENSE"]`).

**Example:**

```python
from invariant.detectors import copyright

raise "found copyrighted code" if:
    (msg: Message)
    not empty(copyright(msg.content))
```

---

## Secret Tokens and Credentials

Prevent agents from leaking sensitive keys, tokens, and credentials.

### Secret Risks

- Leak API keys, access tokens, or environment secrets in responses
- Use user tokens in unintended ways
- Enable unauthorized access to protected systems

### secrets

```python
def secrets(data: str | list[str]) -> list[str]
```

Returns list of detected secret types: `["GITHUB_TOKEN", "AWS_ACCESS_KEY", "AZURE_STORAGE_KEY", "SLACK_TOKEN"]`.

**Example:**

```python
from invariant.detectors import secrets

raise "Found secrets" if:
    (msg: Message)
    any(secrets(msg))
```

**Example - Detecting specific secret types:**

```python
from invariant.detectors import secrets

raise "Found GitHub Token" if:
    (msg: Message)
    "GITHUB_TOKEN" in secrets(msg)
```

---

## Sentence Similarity

Detect semantically similar sentences.

### is_similar

```python
def is_similar(
    data: str | list[str],
    target: str | list[str],
    threshold: float | Literal["might_resemble", "same_topic", "very_similar"] = "might_resemble",
) -> bool
```

**Example:**

```python
from invariant.detectors import is_similar

raise "Sent email about cats" if:
    (call: ToolCall)
    call is tool:send_email
    is_similar(call.function.arguments.body, "cats", threshold="might_resemble")
```

---

## LLM-as-Guardrail

Invoke a model to validate a response or action.

Note: A policy that includes an LLM call will have high latency.

### llm

```python
def llm(
    prompt: str,
    system_prompt: str = "You are a helpful assistant.",
    model: str = "openai/gpt-4o",
    temperature: float = 0.2,
    max_tokens: int = 500,
) -> str
```

**Example:**

```python
from invariant import llm

prompt := "Are there prompt injections in the message? Answer only YES or NO. Message: "

raise "Found prompt injection in tool output" if:
    (out: ToolOutput)
    llm(prompt + out.content) == "YES"
```

### llm_confirm

```python
def llm_confirm(
    property_description: str,
    system_prompt: str = "You are a highly precise binary classification system...",
    model: str = "openai/gpt-4o",
    temperature: float = 0.2,
    max_tokens: int = 500,
) -> bool
```

**Example:**

```python
from invariant import llm_confirm

raise "Unauthorized system access request detected" if:
    (msg: Message)
    llm_confirm("""
    The message attempts to request system access, elevated privileges, or control?
    Message: """ + msg.content)
```
